{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ce0b5a-cb7d-45ac-b190-37e276d54539",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 順伝播\n",
    "\n",
    "## データ\n",
    "$N$次元，$M$個のデータを考える．（$X \\in \\mathbb{R}^{M \\times N}$と書く．）\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_N \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "（例）$3$科目，$5$人のデータ\n",
    "$$\n",
    "X = \n",
    "        \\begin{pmatrix}\n",
    "        国語の点数 & \n",
    "        数学の点数 & \n",
    "        英語の点数 \n",
    "        \\end{pmatrix} = \n",
    "        \\begin{pmatrix}\n",
    "        66 & 67 & 78 \\\\\n",
    "        79 & 88 & 82 \\\\\n",
    "        88 & 80 & 70 \\\\\n",
    "        74 & 66 & 70 \\\\\n",
    "        70 & 85 & 83 \\\\\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "## 入力層から１個のノードへの順伝播\n",
    "出力を$\\mathbf{u}_1^{(1)}$とする．（$\\mathbf{u}_1^{(1)} \\in \\mathbb{R}^{M \\times 1}$）\n",
    "\n",
    "$X$への下のような重み（パラメータ）付き線形和を考える．\n",
    "\n",
    "パラメータ$\\mathbf{w}_1^{(1)} \\in \\mathbb{R}^{N \\times 1}，\\mathbf{b}_1^{(1)} \\in \\mathbb{R}^{M \\times 1}$を\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{u}_1^{(1)} &= w_{11}^{(1)}\\mathbf{x}_1+w_{12}^{(1)}\\mathbf{x}_2+\\cdots+w_{1N}^{(1)}\\mathbf{x}_N +\\mathbf{b}_1^{(1)} \\\\\n",
    "&= X\\mathbf{w}_1^{(1)}+\\mathbf{b}_1^{(1)}\n",
    "\\end{align}\n",
    "$$\n",
    "![1](images/1.png)\n",
    "\n",
    "つまり，テストの点数を例に出すと，次のような式を考えている．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{u}_1^{(1)} &= w_{11}^{(1)}\\mathbf{x}_{国語の点数}+w_{12}^{(1)}\\mathbf{x}_{数学の点数}+w_{13}^{(1)}\\mathbf{x}_{英語の点数} + \\mathbf{b}^{(1)}\\\\\n",
    "\\\\\n",
    "&= w_{11}^{(1)}\\begin{pmatrix}\n",
    "        66 \\\\\n",
    "        79 \\\\\n",
    "        88 \\\\\n",
    "        74 \\\\\n",
    "        70 \\\\\n",
    "        \\end{pmatrix}\n",
    "  +w_{12}^{(1)}\\begin{pmatrix}\n",
    "        67 \\\\\n",
    "        88 \\\\\n",
    "        80 \\\\\n",
    "        66 \\\\\n",
    "        85 \\\\\n",
    "        \\end{pmatrix}\n",
    "  +w_{13}^{(1)}\\begin{pmatrix}\n",
    "        78 \\\\\n",
    "        82 \\\\\n",
    "        70 \\\\\n",
    "        70 \\\\\n",
    "        83 \\\\\n",
    "        \\end{pmatrix}\n",
    "  +\\mathbf{b}^{(1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 入力層からK個のノードへの順伝播\n",
    "出力を$U$とする．（$U^{(1)} \\in \\mathbb{R}^{M \\times K}$）\n",
    "\n",
    "今度は，上で行なった線形和をK個のノードについて考える．（下記）\n",
    "\n",
    "パラメータ$W^{(1)} \\in \\mathbb{R}^{N \\times K}，B^{(1)} \\in \\mathbb{R}^{M \\times K}$，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{w}_1^{(1)} & \\mathbf{w}_2^{(1)} & \\cdots & \\mathbf{w}_K^{(1)}\n",
    "\\end{pmatrix} \\\\\n",
    "B^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{b}_1^{(1)} & \\mathbf{b}_2^{(1)} & \\cdots & \\mathbf{b}_K^{(1)}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "として，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{u}_1^{(1)} &= w_{11}^{(1)}\\mathbf{x}_1+w_{12}^{(1)}\\mathbf{x}_2+\\cdots+w_{1N}^{(1)}\\mathbf{x}_N +\\mathbf{b}_1^{(1)} \\\\\n",
    "&= X\\mathbf{w}_1^{(1)}+\\mathbf{b}_1^{(1)}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{u}_2^{(1)} &= w_{21}^{(1)}\\mathbf{x}_1+w_{22}^{(1)}\\mathbf{x}_2+\\cdots+w_{2N}^{(1)}\\mathbf{x}_N+\\mathbf{b}_2^{(1)} \\\\\n",
    "&= X\\mathbf{w}_2^{(1)}+\\mathbf{b}_2^{(1)}\n",
    "\\end{align} \\\\\n",
    "\\vdots \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{u}_K^{(1)} &= w_{K1}^{(1)}\\mathbf{x}_1+w_{K2}^{(1)}\\mathbf{x}_2+\\cdots+w_{KN}^{(1)}\\mathbf{x}_N+\\mathbf{b}_K^{(1)} \\\\\n",
    "&= X\\mathbf{w}_K^{(1)}+\\mathbf{b}_K^{(1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "であるから，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U^{(1)} &= \n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{u}_1^{(1)} & \\mathbf{u}_2^{(1)} & \\cdots & \\mathbf{u}_K^{(1)}\n",
    "\\end{pmatrix} \\\\\n",
    "&= XW^{(1)}+B^{(1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "上記でやったことは，下記の図に基づく計算．\n",
    "\n",
    "![2](images/2.png)\n",
    "\n",
    "## 活性化関数を通す\n",
    "\n",
    "上記の$U$を活性化関数$f(・)$に通す．\n",
    "すなわち，$H \\in \\mathbb{R}^{M \\times K}$として，\n",
    "\n",
    "$$\n",
    "H = f(U) \n",
    "$$\n",
    "\n",
    "である．（多少雑な表記）\n",
    "\n",
    "後は，この操作を繰り返せば順伝播を定義できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98bf7a8-886c-4cb6-bed6-785c1672b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DataLoader import DataLoader as DL\n",
    "import ActivateFunction as AF\n",
    "import DerivativeAF as DAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fff60b86-3c30-4101-b7ec-6002e1f34af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.65684223e-05]\n",
      " [-9.84647657e-05]\n",
      " [ 5.57165742e-05]\n",
      " ...\n",
      " [ 3.90182516e-04]\n",
      " [ 3.49068124e-05]\n",
      " [ 7.37962654e-04]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    全結合層を定義\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, function):\n",
    "        \"\"\"\n",
    "        パラメータの初期化\n",
    "        \"\"\"\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
    "                                   size=(in_dim, out_dim)).astype(\"float64\")\n",
    "        self.b = np.zeros(out_dim).astype(\"float64\")     \n",
    "        self.function = function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.u = x @ self.W + self.b\n",
    "        h = self.function(self.u)\n",
    "        return h\n",
    "\n",
    "# データ数と、次元を定義\n",
    "m_data = 10000\n",
    "in_dim = 10\n",
    "out_dim = 1\n",
    "\n",
    "# 定義されたデータ数と次元数のデータ（説明変数）を生成\n",
    "x = np.random.randn(m_data, in_dim)\n",
    "\n",
    "# パラメータを適当な値に設定．\n",
    "w = np.random.randn(in_dim, out_dim)\n",
    "b = np.ones((m_data,1))\n",
    "\n",
    "# 目的変数を生成．\n",
    "y = x @ w + b\n",
    "\n",
    "# データを分割するクラスを呼び出す．\n",
    "# y.shape[0]は全データを指定するので，バッチ学習\n",
    "dl = DL(x, batch_size=y.shape[0])\n",
    "\n",
    "# 全結合層のインスタンスを生成\n",
    "input_layer = Dense(in_dim, 4, AF.identity)\n",
    "hidden_layer1 = Dense(4, 5, AF.ReLU)\n",
    "out_layer = Dense(5, out_dim, AF.identity)\n",
    "\n",
    "# 順伝播を試してみる．\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    for data in dl:\n",
    "        print(out_layer(hidden_layer1(input_layer(data))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c4399-5554-47bd-8a21-7d1cc49c4682",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "先ほどの結果から，モデルの出力結果を得た．\n",
    "\n",
    "そこで，そのモデルの出力結果と正解（真の値）とを見比べて，パラメータを更新することを考える．\n",
    "\n",
    "## 誤差関数\n",
    "\n",
    "下図（中間層１層のNN）のように得られた誤差関数を$J$とする．\n",
    "\n",
    "![3.png](images/3.png)\n",
    "\n",
    "## パラメータの更新\n",
    "\n",
    "パラメータの更新については，勾配降下法を使う．\n",
    "\n",
    "\n",
    "## 誤差関数$J$の$W^{OUT}$による偏微分\n",
    "\n",
    "これは，今まで通り誤差関数を出力層に向かう重みで偏微分して，求めれば良い．\n",
    "\n",
    "![4.png](images/4.png)\n",
    "$$\n",
    "\\frac{\\partial{J}}{\\partial{w}_{kj}^{(OUT)}} = \\frac{\\partial J}{\\partial t} \\frac{\\partial t}{\\partial{w}_{kj}^{(OUT)}} = \\frac{\\partial J}{\\partial t} h_k^{(1)}\n",
    "$$\n",
    "## 誤差関数$J$の$W^{1}$による偏微分\n",
    "\n",
    "各パラメータによる誤差関数の偏微分を求める．\n",
    "\n",
    "しかし，入力層に近づくほどネストされるので，求めるのが困難になる．→連鎖律による誤差逆伝播法を用いる．\n",
    "\n",
    "![5.png](images/5.png)\n",
    "\n",
    "ある層$l$の偏微分を\n",
    "$$\n",
    "\\frac{\\partial{J}}{\\partial{w}_{kj}^{(l)}}\n",
    "$$\n",
    "\n",
    "とする．\n",
    "\n",
    "この時，連鎖律より，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{J}}{\\partial{w}_{kj}^{(l)}}=\\frac{\\partial{J}}{\\partial{u}_{k}^{(l)}}\\frac{\\partial{u}_{k}^{(l)}}{\\partial{w}_{kj}^{(l)}}=\\delta_{k}^l h_k^{(l-1)}\n",
    "$$\n",
    "\n",
    "さらに，\n",
    "$$\n",
    "\\delta_{k}^l=\\frac{\\partial{J}}{\\partial{u}_{k}^{(l)}}=\\frac{\\partial{J}}{\\partial{h}_{k}^{(l)}}\\frac{\\partial{h}_{k}^{(l)}}{\\partial{u}_{k}^{(l)}}\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "$$\n",
    "\\frac{\\partial{h}_{k}^{(l)}}{\\partial{u}_{k}^{(l)}} = f^\\prime({u}_{k}^{(l)})\n",
    "$$\n",
    "また，\n",
    "\n",
    "$$\n",
    "\\delta_{k}^l = \\sum_{k=1}^{k=K} \\frac{\\partial J}{\\partial u_k^{(l+1)}} \\frac{\\partial u_k^{(l+1)}}{\\partial h_j^{(l)}} \\frac{\\partial h_j^{(l)}}{\\partial u_j^{(l)}}\n",
    "$$\n",
    "\n",
    "よって，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial u_k^{(l+1)}} = \\delta_{k}^{(l+1)} \\\\\n",
    "\\frac{\\partial h_j^{(l)}}{\\partial u_j^{(l)}} = f^{\\prime}(u_j^{(l)}) \\\\\n",
    "\\frac{\\partial u_k^{(l+1)}}{\\partial h_j^{(l)}} = w_{kj}^{(l+1)}\n",
    "$$\n",
    "\n",
    "つまり，\n",
    "$$\n",
    "\\delta_{k}^{(l)} = f^{\\prime}(u_j^{(l)}) \\sum_{k=1}^{k=K} \\delta_{k}^{(l+1)}w_{kj}^{(l+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10f2f3aa-7c3a-4640-b6e1-9b4c128c01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    全結合層を定義\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
    "        \"\"\"\n",
    "        パラメータの初期化\n",
    "        \"\"\"\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
    "                                   size=(in_dim, out_dim)).astype(\"float64\")\n",
    "        self.b = np.zeros(out_dim).astype(\"float64\")     \n",
    "        self.function = function\n",
    "        self.deriv_function = deriv_function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.u = x @ self.W + self.b\n",
    "        h = self.function(self.u)\n",
    "        return h\n",
    "    def back_propagation(self, delta, W):\n",
    "        \"\"\"\n",
    "        誤差逆伝播（l+1層目の誤差から，l層目の誤差を求める）\n",
    "        \"\"\"\n",
    "        self.delta = (self.deriv_function(self.u) * delta) @ W.T\n",
    "        return self.delta\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        \"\"\"\n",
    "        求めた誤差から，勾配を計算\n",
    "        \"\"\"\n",
    "        batch_size = self.delta.shape[0]\n",
    "\n",
    "        self.dW = self.x.T @ self.delta / batch_size\n",
    "        self.db = np.ones(batch_size) @ self.delta / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4bb662a3-b9f1-4bdd-aac9-7762c3e4ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "ERROR: 0.03775730053558299\n",
      "EPOCH: 100\n",
      "ERROR: 0.03776193584967269\n",
      "EPOCH: 200\n",
      "ERROR: 0.037762043030243955\n",
      "EPOCH: 300\n",
      "ERROR: 0.037762047314650306\n",
      "EPOCH: 400\n",
      "ERROR: 0.037762047424122355\n",
      "EPOCH: 500\n",
      "ERROR: 0.0377620475042307\n",
      "EPOCH: 600\n",
      "ERROR: 0.037762047576130775\n",
      "EPOCH: 700\n",
      "ERROR: 0.03776204752469007\n",
      "EPOCH: 800\n",
      "ERROR: 0.03776204770230614\n",
      "EPOCH: 900\n",
      "ERROR: 0.037762047734421864\n"
     ]
    }
   ],
   "source": [
    "# Denseを用いて計算\n",
    "\n",
    "alpha = 0.01\n",
    "batch_size = y.shape[0]\n",
    "\n",
    "\n",
    "# 全結合層のインスタンスを生成\n",
    "input_layer = Dense(in_dim, 4, function=AF.ReLU, deriv_function=DAF.deriv_relu)\n",
    "hidden_layer1 = Dense(4, 5, function=AF.ReLU, deriv_function=DAF.deriv_relu)\n",
    "out_layer = Dense(5, out_dim, function=AF.identity, deriv_function=DAF.deriv_identity)\n",
    "\n",
    "# まずは，順伝播．\n",
    "dl_x = DL(x, batch_size=100)\n",
    "dl_y = DL(y, batch_size=100)\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for x, y in zip(dl_x, dl_y):\n",
    "        t = out_layer(hidden_layer1(input_layer(x)))\n",
    "        # 出力層の誤差を計算．\n",
    "        delta_out = y - t\n",
    "        # 各層に誤差を逆伝播．\n",
    "        out_layer.back_propagation(delta_out, out_layer.W)\n",
    "        hidden_layer1.back_propagation(out_layer.delta, hidden_layer1.W)\n",
    "        input_layer.back_propagation(hidden_layer1.delta, input_layer.W)\n",
    "        \n",
    "        # 各層の勾配を計算\n",
    "        out_layer.compute_grad()\n",
    "        hidden_layer1.compute_grad()\n",
    "        input_layer.compute_grad()\n",
    "        \n",
    "        # 勾配に基づいて，各層をupdate\n",
    "        out_layer.W -= alpha * out_layer.W\n",
    "        hidden_layer1.W -= alpha * hidden_layer1.W\n",
    "        input_layer.W -= alpha * input_layer.W\n",
    "        \n",
    "    if epoch == 300:\n",
    "        alpha /= 10\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        print(f\"ERROR: {np.linalg.norm(y - out_layer(hidden_layer1(input_layer(x))))**2/(2*m_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
